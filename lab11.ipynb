{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task1"
      ],
      "metadata": {
        "id": "G8FR-x1bmdUj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GYJaCcldTV"
      },
      "outputs": [],
      "source": [
        "# takes lot of time since environment is 40x40.\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# hyperparameters\n",
        "alpha = 0.8\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "num_states = 40\n",
        "num_actions = 3  # left, stay, right\n",
        "q_table = np.zeros((num_states, num_states, num_actions))\n",
        "\n",
        "# convert continuous state space into discrete state space\n",
        "def discretize_state(state):\n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    env_distance = (env_high - env_low) / num_states\n",
        "    pos = int((state[0] - env_low[0]) / env_distance[0])\n",
        "    vel = int((state[1] - env_low[1]) / env_distance[1])\n",
        "    return pos, vel\n",
        "\n",
        "# training\n",
        "num_episodes = 10000\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    pos, vel = discretize_state(state)\n",
        "    done = False\n",
        "    while not done:\n",
        "        # exploration vs exploitation\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[pos][vel])\n",
        "        # take action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_pos, next_vel = discretize_state(next_state)\n",
        "        # update Q-value\n",
        "        q_table[pos][vel][action] = q_table[pos][vel][action] + alpha * (reward + gamma * np.max(q_table[next_pos][next_vel]) - q_table[pos][vel][action])\n",
        "        pos, vel = next_pos, next_vel\n",
        "\n",
        "# testing\n",
        "total_reward = 0\n",
        "num_trials = 100\n",
        "for i in range(num_trials):\n",
        "    state = env.reset()\n",
        "    pos, vel = discretize_state(state)\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[pos][vel])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        pos, vel = discretize_state(next_state)\n",
        "        print(\"State:\", next_state, \"Action:\", action, \"Reward:\", reward)\n",
        "print(\"Average reward per trial:\", total_reward / num_trials)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task2"
      ],
      "metadata": {
        "id": "dr10cyM-mgfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# Set up the Q-learning agent\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "lr = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.1\n",
        "\n",
        "# Define the reward function\n",
        "def get_reward(state, action, next_state):\n",
        "    if next_state == 4:\n",
        "        return 10\n",
        "    elif state[0] == next_state[2] and state[1] == next_state[3] and action == 5:\n",
        "        return -10\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Train the agent\n",
        "for i in range(10000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\\\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        Q[state][action] += lr * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "        state = next_state\n",
        "\n",
        "# Test the agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "\n",
        "    action = np.argmax(Q[state])\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    # Print the current state, action, reward, and next state\n",
        "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward, \"Next state:\", next_state)\n",
        "    state = next_state\n"
      ],
      "metadata": {
        "id": "tgQg12_hmhoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task3"
      ],
      "metadata": {
        "id": "NOAn3VmdmoGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the state space\n",
        "state_space = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Define the action space\n",
        "action_space = ['transmit', 'conserve_energy']\n",
        "\n",
        "# Define the reward function\n",
        "positive_reward = 1\n",
        "negative_reward = -1\n",
        "threshold = 0.5\n",
        "\n",
        "def get_reward(action, cost):\n",
        "    if action == 'transmit' and cost < threshold:\n",
        "        return positive_reward\n",
        "    elif action == 'transmit' and cost >= threshold:\n",
        "        return negative_reward\n",
        "    else:\n",
        "        return positive_reward\n",
        "\n",
        "# Define the Q-learning algorithm\n",
        "def q_learning(state_space, action_space, get_reward, alpha, gamma, epsilon, max_iterations):\n",
        "\n",
        "    q_values = np.zeros((len(state_space), len(action_space)))\n",
        "    initial_state = state_space[0]\n",
        "    current_state = initial_state\n",
        "\n",
        "    def execute_action(action):\n",
        "        # Simulate the cost of the action\n",
        "        if action == 'transmit':\n",
        "            cost = np.random.uniform(0, 1)\n",
        "        else:\n",
        "            cost = 0\n",
        "\n",
        "        # Simulate the next state\n",
        "        if current_state == state_space[-1]:\n",
        "            next_state = current_state\n",
        "        else:\n",
        "            next_state = state_space[state_space.index(current_state) + 1]\n",
        "\n",
        "        return next_state, cost\n",
        "\n",
        "    # Initialize the total reward\n",
        "    total_reward = 0\n",
        "\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Choose an action using epsilon-greedy policy\n",
        "        if np.random.uniform() < epsilon:\n",
        "            action = np.random.choice(action_space)\n",
        "        else:\n",
        "            action = action_space[np.argmax(q_values[current_state])]\n",
        "\n",
        "        next_state, cost = execute_action(action)\n",
        "        reward = get_reward(action, cost)\n",
        "        q_values[current_state, action_space.index(action)] += alpha * (reward + gamma * np.max(q_values[next_state]) - q_values[current_state, action_space.index(action)])\n",
        "\n",
        "        # Update the total reward and current state\n",
        "        total_reward += reward\n",
        "        current_state = next_state\n",
        "\n",
        "    # Return the optimal policy\n",
        "    policy = {}\n",
        "    for state in state_space:\n",
        "        policy[state] = action_space[np.argmax(q_values[state])]\n",
        "\n",
        "    return policy\n",
        "\n",
        "policy = q_learning(state_space=state_space,\n",
        "                    action_space=action_space,\n",
        "                    get_reward=get_reward,\n",
        "                    alpha=0.1,\n",
        "                    gamma=0.9,\n",
        "                    epsilon=0.1,\n",
        "                    max_iterations=10000)\n",
        "\n",
        "print(policy)"
      ],
      "metadata": {
        "id": "M7IZTBgmmnzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}